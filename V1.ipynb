{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader, random_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing entropy loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PartialCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self, ignore_index=None):\n",
    "        super(PartialCrossEntropyLoss, self).__init__()\n",
    "        self.ignore_index = ignore_index\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        if self.ignore_index is not None:\n",
    "            mask = targets != self.ignore_index\n",
    "            inputs = inputs[mask]\n",
    "            targets = targets[mask]\n",
    "        return nn.functional.cross_entropy(inputs, targets)\n",
    "    \n",
    "\n",
    "# inputs = torch.randn(3, 5, requires_grad=True)  # Batch of 3, 5 classes\n",
    "# targets = torch.tensor([1, 0, 4])  # Corresponding targets\n",
    "# ignore_index = -1  # Example of an index to ignore\n",
    "\n",
    "# criterion = PartialCrossEntropyLoss(ignore_index=ignore_index)\n",
    "# loss = criterion(inputs, targets)\n",
    "# print('Loss:', loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using my dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoadDataset(Dataset):\n",
    "    def __init__(self, image_dir, centerline_dir, label_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.centerline_dir = centerline_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.transform = transform\n",
    "        self.image_names = sorted(os.listdir(image_dir))  # This will list files like 'image160.bmp'\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_filename = self.image_names[idx]\n",
    "        base_name = image_filename.replace('image', '').replace('.bmp', '')\n",
    "        \n",
    "        image_path = os.path.join(self.image_dir, f'image{base_name}.bmp')\n",
    "        centerline_path = os.path.join(self.centerline_dir, f'new_line{base_name}.bmp')\n",
    "        label_path = os.path.join(self.label_dir, f'{base_name}.bmp')\n",
    "\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        centerline = Image.open(centerline_path).convert('RGB')\n",
    "        label = Image.open(label_path).convert('L')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            centerline = self.transform(centerline)\n",
    "            label = self.transform(label)\n",
    "\n",
    "        return image, centerline, label\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# dataset = RoadDataset('./Train/image', './Train/centerline', './Train/label', transform=transform)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=1):\n",
    "        super(UNet, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, out_channels, kernel_size=3, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.encoder(x)\n",
    "        x = self.decoder(x1)\n",
    "        x = self.upsample(x)\n",
    "        return x\n",
    "    \n",
    "# model = UNet(in_channels=3, out_channels=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 354501.125\n",
      "Epoch 2, Loss: 305896.65625\n",
      "Epoch 3, Loss: 371656.375\n",
      "Epoch 4, Loss: 339111.0625\n",
      "Epoch 5, Loss: 333065.09375\n",
      "Test Loss with LR=0.01, Batch Size=4: 277759.53125\n",
      "Test Loss with LR=0.01, Batch Size=4: 261847.53125\n",
      "Test Loss with LR=0.01, Batch Size=4: 255384.390625\n",
      "Test Loss with LR=0.01, Batch Size=4: 297801.21875\n",
      "Test Loss with LR=0.01, Batch Size=4: 417157.46875\n",
      "Test Loss with LR=0.01, Batch Size=4: 389102.78125\n",
      "Test Loss with LR=0.01, Batch Size=4: 442484.3125\n",
      "Test Loss with LR=0.01, Batch Size=4: 233657.8125\n",
      "Epoch 1, Loss: 690752.75\n",
      "Epoch 2, Loss: 714409.5\n",
      "Epoch 3, Loss: 769842.5\n",
      "Epoch 4, Loss: 594310.875\n",
      "Epoch 5, Loss: 604569.125\n",
      "Test Loss with LR=0.01, Batch Size=8: 569585.25\n",
      "Test Loss with LR=0.01, Batch Size=8: 583918.125\n",
      "Test Loss with LR=0.01, Batch Size=8: 851052.5\n",
      "Test Loss with LR=0.01, Batch Size=8: 713705.5625\n",
      "Epoch 1, Loss: 1293841.125\n",
      "Epoch 2, Loss: 1400333.0\n",
      "Epoch 3, Loss: 1421643.25\n",
      "Epoch 4, Loss: 1637108.0\n",
      "Epoch 5, Loss: 1643180.0\n",
      "Test Loss with LR=0.01, Batch Size=16: 1214214.0\n",
      "Test Loss with LR=0.01, Batch Size=16: 1647113.75\n",
      "Epoch 1, Loss: 307378.03125\n",
      "Epoch 2, Loss: 311026.625\n",
      "Epoch 3, Loss: 350940.0\n",
      "Epoch 4, Loss: 339244.5625\n",
      "Epoch 5, Loss: 320366.375\n",
      "Test Loss with LR=0.001, Batch Size=4: 269291.71875\n",
      "Test Loss with LR=0.001, Batch Size=4: 254194.796875\n",
      "Test Loss with LR=0.001, Batch Size=4: 244004.625\n",
      "Test Loss with LR=0.001, Batch Size=4: 286554.9375\n",
      "Test Loss with LR=0.001, Batch Size=4: 401373.75\n",
      "Test Loss with LR=0.001, Batch Size=4: 373949.3125\n",
      "Test Loss with LR=0.001, Batch Size=4: 432694.09375\n",
      "Test Loss with LR=0.001, Batch Size=4: 224709.140625\n",
      "Epoch 1, Loss: 680742.4375\n",
      "Epoch 2, Loss: 768615.5\n",
      "Epoch 3, Loss: 626339.3125\n",
      "Epoch 4, Loss: 666969.875\n",
      "Epoch 5, Loss: 756998.375\n",
      "Test Loss with LR=0.001, Batch Size=8: 555091.125\n",
      "Test Loss with LR=0.001, Batch Size=8: 567644.4375\n",
      "Test Loss with LR=0.001, Batch Size=8: 832148.5625\n",
      "Test Loss with LR=0.001, Batch Size=8: 699247.375\n",
      "Epoch 1, Loss: 1597688.375\n",
      "Epoch 2, Loss: 1460715.25\n",
      "Epoch 3, Loss: 1592496.125\n",
      "Epoch 4, Loss: 1261285.25\n",
      "Epoch 5, Loss: 1471712.625\n",
      "Test Loss with LR=0.001, Batch Size=16: 1186513.25\n",
      "Test Loss with LR=0.001, Batch Size=16: 1616655.125\n",
      "Epoch 1, Loss: 307005.0\n",
      "Epoch 2, Loss: 425638.84375\n",
      "Epoch 3, Loss: 293776.25\n",
      "Epoch 4, Loss: 283648.65625\n",
      "Epoch 5, Loss: 296982.9375\n",
      "Test Loss with LR=0.0001, Batch Size=4: 270604.78125\n",
      "Test Loss with LR=0.0001, Batch Size=4: 254935.65625\n",
      "Test Loss with LR=0.0001, Batch Size=4: 247287.25\n",
      "Test Loss with LR=0.0001, Batch Size=4: 288026.375\n",
      "Test Loss with LR=0.0001, Batch Size=4: 406459.53125\n",
      "Test Loss with LR=0.0001, Batch Size=4: 377885.375\n",
      "Test Loss with LR=0.0001, Batch Size=4: 438047.96875\n",
      "Test Loss with LR=0.0001, Batch Size=4: 225144.96875\n",
      "Epoch 1, Loss: 788673.0\n",
      "Epoch 2, Loss: 673930.9375\n",
      "Epoch 3, Loss: 663610.8125\n",
      "Epoch 4, Loss: 610025.5\n",
      "Epoch 5, Loss: 643101.5\n",
      "Test Loss with LR=0.0001, Batch Size=8: 559954.0\n",
      "Test Loss with LR=0.0001, Batch Size=8: 571659.1875\n",
      "Test Loss with LR=0.0001, Batch Size=8: 835905.125\n",
      "Test Loss with LR=0.0001, Batch Size=8: 701683.875\n",
      "Epoch 1, Loss: 1538554.625\n",
      "Epoch 2, Loss: 1348046.0\n",
      "Epoch 3, Loss: 1571154.125\n",
      "Epoch 4, Loss: 1390170.125\n",
      "Epoch 5, Loss: 1354010.625\n",
      "Test Loss with LR=0.0001, Batch Size=16: 1208766.5\n",
      "Test Loss with LR=0.0001, Batch Size=16: 1639852.625\n"
     ]
    }
   ],
   "source": [
    "image_dir = './Train/image'\n",
    "centerline_dir = './Train/centerline'\n",
    "label_dir = './Train/label'\n",
    "\n",
    "dataset = RoadDataset(image_dir, centerline_dir, label_dir, transform=transform)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "learning_rates = [0.01, 0.001, 0.0001]\n",
    "batch_sizes = [4, 8, 16]\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for batch_size in batch_sizes:\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        model = UNet(in_channels=3, out_channels=1)\n",
    "        criterion = PartialCrossEntropyLoss(ignore_index=-1)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "        # 5 epochs\n",
    "        for epoch in range(5):\n",
    "            model.train()\n",
    "            for inputs, _, targets in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs, _, targets in test_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                print(f'Test Loss with LR={lr}, Batch Size={batch_size}: {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Techincal report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method:\n",
    "- I used UNet architecture for road segmentation from satellite images. The UNet model is a convolutional neural network (CNN) commonly used for semantic segmentation tasks due to its ability to capture both global and local features efficiently.\n",
    "\n",
    "- The UNet architecture consists of an encoder-decoder structure. The encoder extracts features from the input image through a series of convolutional layers followed by max-pooling operations, reducing the spatial dimensions while increasing the depth. The decoder then upsamples the features back to the original resolution through transpose convolutions, allowing the network to generate pixel-wise predictions.\n",
    "\n",
    "- Additionally, **RoadDataset**, was created to handle the input images, centerline images, and corresponding labels. The dataset class loads the images and labels, applies any specified transformations, and returns them as tensors.\n",
    "\n",
    "- For training, the **Adam optimizer** was utilized with different learning rates (LR) ranging from *0.01 to 0.0001*. The partial cross-entropy loss function's objective is optimization. This loss function calculates the cross-entropy only for the pixels labeled as road (ignoring background pixels) to mitigate class imbalance issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment:\n",
    "\n",
    "- #### Purpose:\n",
    "    - The objective of the experiment was to evaluate the performance of the UNet model for road segmentation on satellite images. The experiment aimed to analyze the effect of different learning rates and batch sizes on the model's training and testing performance.\n",
    "\n",
    "- #### Hypothesis:\n",
    "    - It was hypothesized that varying the learning rates and batch sizes would influence the convergence speed and final accuracy of the UNet model. *Higher learning rates might accelerate the training process* but could lead to unstable convergence, while larger batch sizes could provide computational efficiency but might result in degraded performance due to reduced batch diversity.\n",
    "\n",
    "- #### Experimental Process:\n",
    "    - The experiment involved training the UNet model for *5 epochs* on a dataset consisting of satellite images, centerline images, and corresponding road segmentation labels. The dataset was divided into training and testing sets with an *80:20 split*. The model was trained using different combinations of **learning rates** *(0.01, 0.001, 0.0001)* and **batch sizes** *(4, 8, 16)*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results:\n",
    "\n",
    "**Learning Rate (LR) = 0.01:**\n",
    "\n",
    "- Batch Size = 4: Test Losses ranged from 233657.8125 to 442484.3125.\n",
    "- Batch Size = 8: Test Losses ranged from 569585.25 to 851052.5.\n",
    "- Batch Size = 16: Test Losses ranged from 1214214.0 to 1647113.75.\n",
    "\n",
    "**Learning Rate (LR) = 0.001:**\n",
    "\n",
    "- Batch Size = 4: Test Losses ranged from 224709.140625 to 432694.09375.\n",
    "- Batch Size = 8: Test Losses ranged from 555091.125 to 832148.5625.\n",
    "- Batch Size = 16: Test Losses ranged from 1186513.25 to 1616655.125.\n",
    "\n",
    "**Learning Rate (LR) = 0.0001:**\n",
    "\n",
    "- Batch Size = 4: Test Losses ranged from 225144.96875 to 438047.96875.\n",
    "- Batch Size = 8: Test Losses ranged from 559954.0 to 835905.125.\n",
    "- Batch Size = 16: Test Losses ranged from 1208766.5 to 1639852.625."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion:\n",
    "The results demonstrate that both learning rate and batch size significantly impact the model's performance. *Lower learning rates generally ensure more stable training and better convergence but require more time*. Higher learning rates can accelerate convergence but may cause instability. Smaller batch sizes introduce more randomness, potentially improving convergence and generalization but can **add noise**. Larger batch sizes enhance **computational efficiency but might limit parameter space exploration**.\n",
    "\n",
    "Further hyperparameter tuning and exploring alternative optimization techniques could enhance performance and convergence. Adding evaluation metrics like *Intersection over Union (IoU) or F1 score* would offer a more detailed assessment of the model's segmentation accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
